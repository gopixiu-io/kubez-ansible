---
########################
# Kubez-ansible Options
########################
network_interface: "eth0"
api_interface: "{{ network_interface }}"
api_interface_address: "{{ hostvars[inventory_hostname]['ansible_' + api_interface]['ipv4']['address'] }}"

aliyun_repo: "registry.cn-hangzhou.aliyuncs.com/google_containers"
image_repository: "{{ aliyun_repo }}"

enable_registry: "no"
registry_server: 127.0.0.1:4000
registry_namespace: "kubernetes"
registry_repo: "{{ registry_server }}/{{ registry_namespace }}"

#####################
# keepalived options
#####################
# Arbitrary unique number from 0..255
keepalived_virtual_router_id: "68"

#####################
# Kubernetes Options
#####################
kube_application_dir: "/tmp/pixiuspace"
kubez_namespace: pixiu-system

# This should be a VIP, an unused IP on your network that will float between
# the hosts running keepalived for high-availability.
kube_vip_address: ""
kube_vip_port: 6443

dashboard_vip_address: ""
dashboard_node_port: 30001
ingress_node_port: 30006
ingress_tls_node_port: 30008

cluster_cidr: "172.30.0.0/16"
service_cidr: "10.254.0.0/16"

# Kubernetes network cni options
enable_flannel: "{{ not enable_calico | bool }}"
enable_calico: "no"

enable_kubernetes: "yes"
enable_kubernetes_ha: "no"
enable_haproxy: "no"
enable_metrics_server: "yes"
enable_nfs_provisioner: "{{ enable_nfs }}"
enable_rbd_provisioner: "no"
enable_dashboard: "no"
enable_ingress_nginx: "yes"
enable_helm: "yes"

# Addon helm charts
enable_prometheus: "no"
enable_grafana: "{{ enable_prometheus }}"

kube_release: 1.23.6
kube_release_ubuntu: 1.23.6-00
kubernetes_version: "v{{ kube_release }}"

# runtime docker version
docker_release: 20.10.15
# 通过 apt-cache madison docker-ce 查找合适版本
docker_release_ubuntu: 5:20.10.7~3-0~ubuntu-xenial

# runtime containerd version
containerd_release: 1.6.4
containerd_release_ubuntu: 1.5.5-0ubuntu3~18.04.2

helm_release: v3.5.2

node_config_directory: "/etc/kubez/"

kube_repo: "{{ registry_repo if enable_registry | bool else image_repository }}"

kube_applications:
  # kubez-sysns should exists before the other applications created
  - name: kubez-sysns
    enabled: "yes"
  - name: kube-flannel
    enabled: "{{ enable_flannel | bool }}"
  - name: kube-calico
    enabled: "{{ enable_calico | bool }}"
  - name: dashboard
    enabled: "{{ enable_dashboard | bool }}"
  - name: metrics-server
    enabled: "{{ enable_metrics_server | bool }}"
  - name: nfs-provisioner
    enabled: "{{ enable_nfs_provisioner | bool }}"
  - name: rbd-provisioner
    enabled: "{{ enable_rbd_provisioner | bool }}"
  - name: ingress-nginx
    enabled: "{{ enable_ingress_nginx | bool }}"
  - name: kubez-autoscaler
    enabled: "{{ enable_hpav2 | bool}}"

#####################
# Application Images
#####################
dashboard_url: "{{ registry_server + '/kubernetes' if enable_registry | bool else image_repository }}"
dashboard_image: "{{ dashboard_url }}/kubernetes-dashboard-amd64:v1.10.1"

nfs_provisioner_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
nfs_provisioner_image: "{{ nfs_provisioner_url }}/external_storage/nfs-client-provisioner:latest"

rbd_provisioner_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
rbd_provisioner_image: "{{ rbd_provisioner_url }}/external_storage/rbd-provisioner"

fluentd_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
fluentd_image: "{{ fluentd_url }}/fluentd_elasticsearch/fluentd:v2.7.0"

kibana_url: "{{ registry_server if enable_registry | bool else 'docker.elastic.co' }}"
kibana_image: "{{ kibana_url }}/kibana/kibana-oss:7.2.0"

elasticsearch_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
elasticsearch_image: "{{ elasticsearch_url }}/fluentd_elasticsearch/elasticsearch:v7.2.0"

alpine_url: "{{ registry_server + '/fluentd_elasticsearch/' if enable_registry | bool else '' }}"
alpine_image: "{{ alpine_url }}alpine:3.6"

helm_url: "{{ registry_server + '/' if enable_registry | bool else '' }}"
helm_image: "{{ helm_url }}jacky06/helm-toolbox:{{ helm_release }}"

#######################
# StorageClass Options
#######################
enable_nfs: "yes"

nfs_volume: /data/share
nfs_cidr: "*"

pool_name: kube
user_id: "{{ pool_name }}"

# Ceph monitors, comma delimited. This parameter is required.
monitors: 172.16.60.102:6789

# ceph auth get-key client.admin | base64
admin_key: QVFDTWhUcGVVUWZrRXhBQUwyVTNMdTdQSk5WRkxUMTczb3ovcFE9PQ==

# ceph osd pool create pool_name 8 8
# ceph auth add client.pool_name mon 'allow r' osd 'allow rwx pool=pool_name'
# ceph auth get-key client.pool_name | base64
ceph_key: QVFCdzN6NWVGMjJCTFJBQVcvMkU2a051UW1JSHU1VTRXZ2ZEd3c9PQ==

#####################
# Prometheus Options
#####################
# https://github.com/prometheus-operator/kube-prometheus
# version is v0.6.0
# grafana will also be deploy when prometheus is enable.

################################
# Fluentd-elasticsearch Options
################################
# https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/README.md
# The Fluentd, elasticsearch, and kibana will be installed when enabled.
enable_fluentd_elasticsearch: "no"

###########################
# Kubez-autoscaler Options
###########################
enable_hpav2: "no"

##################
# Jenkins Options
##################
enable_jenkins: "no"

jenkins_namespace: "{{ kubez_namespace }}"
jenkins_storage_class: managed-nfs-storage
jenkins_storage_size: "8Gi"

# The initial password for admin
initial_admin_password: "admin123456"

#################
# Harbor Options
#################
enable_harbor: "no"
harbor_name: harbor
harbor_namespace: "{{ kubez_namespace }}"
harbor_repository:
  name: harbor
  url: https://helm.goharbor.io
expose_http_nodeport: 30011
expose_notary_nodeport: 30012

# Setting it to "keep" to avoid removing PVCs during a helm delete
# operation. Leaving it empty will delete PVCs after the chart deleted
# (this does not apply for PVCs that are created for internal database
# and redis components, i.e. they are never deleted automatically)
harbor_resource_policy: " "

# Valid options are [ ingress, nodePort ]
expose_type: nodePort
expose_core_domain: core.harbor.kubez.com
expose_notary_domain: notary.harbor.kubez.com
harbor_storage_class: managed-nfs-storage

##################
# Mariadb Options
##################
enable_mariadb: "no"
mariadb_name: mariadb
mariadb_namespace: "{{ kubez_namespace }}"
mariadb_storage_class: managed-nfs-storage

###############
# Kong Options
###############
enable_kong: "no"
kong_name: kong
kong_namespace: "{{ kubez_namespace }}"
kong_storage_class: managed-nfs-storage

#################
# Redis Options
#################
enable_redis: "no"
redis_name: redis
redis_namespace: "{{ kubez_namespace }}"
redis_storage_class: managed-nfs-storage

#################
# Consul Options
#################
enable_consul: "no"
consul_name: consul
consul_namespace: "{{ kubez_namespace }}"
consul_storage_class: managed-nfs-storage
consul_service_type: "NodePort"
consul_service_nodeport: "30014"

##########################
# Helm Chart Applications
##########################
# TODO: 重构
chart_applications:
  # harbor
  - name: "{{ harbor_name }}"
    namespace: "{{ harbor_namespace }}"
    chart: harbor
    release_enabled: "{{ enable_harbor }}"
    repository: "{{ harbor_repository }}"
    chart_extra_vars:
      expose.type: "{{ expose_type }}"
      expose.tls.enabled: 'false'
      expose.nodePort.ports.http.nodePort: "{{ '' if expose_type == 'ingress' else expose_http_nodeport }}"
      expose.nodePort.ports.notary.nodePort: "{{ '' if expose_type == 'ingress' else expose_notary_nodeport }}"
      expose.ingress.hosts.core: "{{ expose_core_domain if expose_type == 'ingress' else '' }}"
      expose.ingress.hosts.notary: "{{ expose_notary_domain if expose_type == 'ingress' else '' }}"
      persistence.persistentVolumeClaim.registry.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.chartmuseum.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.jobservice.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.database.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.redis.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.trivy.storageClass: "{{ harbor_storage_class }}"
      persistence.resourcePolicy: "{{ harbor_resource_policy }}"
      externalURL: "{{ 'https://' if expose_type == 'ingress' else 'http://' }}{{ expose_core_domain if expose_type == 'ingress' else hostvars[groups['kube-master'][0]]['ansible_' + network_interface]['ipv4']['address'] }}{{ '' if expose_type == 'ingress' else ':' }}{{ '' if expose_type == 'ingress' else expose_http_nodeport }}"

enable_charts:
  - name: prometheus
    enabled: "{{ enable_prometheus | bool }}"
  - name: grafana
    enabled: "{{ enable_grafana | bool }}"
  - name: jenkins
    enabled: "{{ enable_jenkins | bool }}"
  - name: mariadb
    enabled: "{{ enable_mariadb | bool }}"
  - name: kong
    enabled: "{{ enable_kong | bool }}"
  - name: redis
    enabled: "{{ enable_redis | bool }}"
  - name: consul
    enabled: "{{ enable_consul | bool }}"

charts:
  prometheus:
    name: prometheus
    namespace: "{{ kubez_namespace }}"
    repository:
      name: prometheus-community
      url: https://prometheus-community.github.io/helm-charts
    chart:
      path: prometheus-community/prometheus
      version: 15.9.2
    chart_extra_vars:
      server.persistentVolume.enabled: 'false' # 必须是字符串格式，否则会被 helm_toolbox 模块忽略
      alertmanager.persistentVolume.enabled: 'false'
      # 以 kube-state-metrics 为前缀设置子 chart kube-state-metrics 的属性
      kube-state-metrics.image.repository: jacky06/kube-state-metrics

  grafana:
    name: grafana
    namespace: "{{ kubez_namespace }}"
    repository:
      name: grafana
      url: https://grafana.github.io/helm-charts
    chart:
      path: grafana/grafana
      version: 6.29.6
    chart_extra_vars: {} # 如果没有 extra_vars，需留空

  jenkins:
    name: jenkins
    namespace: "{{ jenkins_namespace }}"
    repository:
      name: jenkinsci
      url: https://charts.jenkins.io/
    chart:
      path: jenkinsci/jenkins
      version: 4.2.20
    chart_extra_vars:
      persistence.storageClass: "{{ jenkins_storage_class }}"
      persistence.size: "{{ jenkins_storage_size }}"
      controller.adminPassword: "{{ initial_admin_password }}"

  mariadb:
    name: "{{ mariadb_name }}"
    namespace: "{{ mariadb_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/mariadb
      version: 11.4.3
    chart_extra_vars:
      global.storageClass: "{{ mariadb_storage_class }}"
      primary.persistence.storageClass: "{{ mariadb_storage_class }}"
      secondary.persistence.storageClass: "{{ mariadb_storage_class }}"

  kong:
    name: "{{ kong_name }}"
    namespace: "{{ kong_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/kong
      version: 9.0.1
    chart_extra_vars: {}

  redis:
    name: "{{ redis_name }}"
    namespace: "{{ redis_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/redis
      version: 17.4.2
    chart_extra_vars:
      global.storageClass: "{{ redis_storage_class }}"
      master.persistence.storageClass: "{{ redis_storage_class }}"
      slave.persistence.storageClass: "{{ redis_storage_class }}"

  consul:
    name: "{{ consul_name }}"
    namespace: "{{ consul_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/consul
      version: 10.9.9
    chart_extra_vars:
      global.storageClass: "{{ consul_storage_class }}"
      service.type: "{{ consul_service_type }}"
      service.nodePort: "{{ consul_service_nodeport }}"
