---
########################
# Kubez-ansible Options
########################
network_interface: "eth0"
api_interface: "{{ network_interface }}"
api_interface_address: "{{ hostvars[inventory_hostname]['ansible_' + api_interface]['ipv4']['address'] }}"

aliyun_repo: "registry.cn-hangzhou.aliyuncs.com/google_containers"
image_repository: "{{ aliyun_repo }}"

enable_registry: "no"
registry_server: 127.0.0.1:4000
registry_namespace: "kubernetes"
registry_repo: "{{ registry_server }}/{{ registry_namespace }}"

#####################
# keepalived options
#####################
# Arbitrary unique number from 0..255
keepalived_virtual_router_id: "68"

#####################
# Kubernetes Options
#####################
kube_application_dir: "/tmp/pixiuspace"
kubez_namespace: pixiu-system

# This should be a VIP, an unused IP on your network that will float between
# the hosts running keepalived for high-availability.
kube_vip_address: ""
kube_vip_port: 6443

ingress_node_port: 30006
ingress_tls_node_port: 30008

cluster_cidr: "172.30.0.0/16"
service_cidr: "10.254.0.0/16"

# Kubernetes network cni options
enable_flannel: "{{ not enable_calico | bool }}"
enable_calico: "no"

enable_kubernetes: "yes"
enable_kubernetes_ha: "no"
enable_haproxy: "no"
enable_metrics_server: "yes"
enable_nfs_provisioner: "{{ enable_nfs }}"
enable_rbd_provisioner: "no"
enable_ingress_nginx: "yes"
enable_helm: "yes"

# Addon helm charts
kube_release: 1.23.6
kube_release_ubuntu: 1.23.6-00
kubernetes_version: "v{{ kube_release }}"

# runtime docker version
docker_release: 20.10.15
# 通过 apt-cache madison docker-ce 查找合适版本
docker_release_ubuntu: 5:20.10.7~3-0~ubuntu-xenial

# runtime containerd version
containerd_release: 1.6.4
containerd_release_ubuntu: 1.5.5-0ubuntu3~18.04.2

helm_release: 3.9.0

node_config_directory: "/etc/kubez/"

kube_repo: "{{ registry_repo if enable_registry | bool else image_repository }}"

kube_applications:
  # kubez-sysns should exists before the other applications created
  - name: kubez-sysns
    enabled: "yes"
  - name: kube-flannel
    enabled: "{{ enable_flannel | bool }}"
  - name: kube-calico
    enabled: "{{ enable_calico | bool }}"
  - name: metrics-server
    enabled: "{{ enable_metrics_server | bool }}"
  - name: nfs-provisioner
    enabled: "{{ enable_nfs_provisioner | bool }}"
  - name: rbd-provisioner
    enabled: "{{ enable_rbd_provisioner | bool }}"
  - name: ingress-nginx
    enabled: "{{ enable_ingress_nginx | bool }}"
  - name: kubez-autoscaler
    enabled: "{{ enable_hpav2 | bool}}"
  - name: olm
    enabled: "{{ enable_olm | bool}}"

#####################
# Application Images
#####################
nfs_provisioner_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
nfs_provisioner_image: "{{ nfs_provisioner_url }}/external_storage/nfs-client-provisioner:latest"

rbd_provisioner_url: "{{ registry_server if enable_registry | bool else 'quay.io' }}"
rbd_provisioner_image: "{{ rbd_provisioner_url }}/external_storage/rbd-provisioner"

helm_url: "{{ registry_server + '/' if enable_registry | bool else '' }}"
helm_image: "{{ helm_url }}jacky06/helm-toolbox:v{{ helm_release }}"

#######################
# StorageClass Options
#######################
enable_nfs: "yes"

nfs_volume: /data/share
nfs_cidr: "*"

pool_name: kube
user_id: "{{ pool_name }}"

# Ceph monitors, comma delimited. This parameter is required.
monitors: 172.16.60.102:6789

# ceph auth get-key client.admin | base64
admin_key: QVFDTWhUcGVVUWZrRXhBQUwyVTNMdTdQSk5WRkxUMTczb3ovcFE9PQ==

# ceph osd pool create pool_name 8 8
# ceph auth add client.pool_name mon 'allow r' osd 'allow rwx pool=pool_name'
# ceph auth get-key client.pool_name | base64
ceph_key: QVFCdzN6NWVGMjJCTFJBQVcvMkU2a051UW1JSHU1VTRXZ2ZEd3c9PQ==

#####################
# Prometheus Options
#####################
# https://github.com/prometheus-operator/kube-prometheus
# version is v0.6.0
# grafana will also be deploy when prometheus is enable.
enable_prometheus: "no"
enable_grafana: "{{ enable_prometheus }}"

########################
# Elasticsearch Options
########################
# The Fluentd, elasticsearch, and kibana will be installed when enabled.
enable_elasticsearch: "no"
elasticsearch_name: elasticsearch
elasticsearch_namespace: "{{ kubez_namespace }}"
elasticsearch_chart_version: 8.5.1

elasticsearch_replicas: 1 # 生产环境推荐 3 副本
minimum_master_nodes: 1 # 生产环境至少为 2 节点

###################
# Filebeat Options
###################
enable_filebeat: "no"
filebeat_name: filebeat
filebeat_namespace: "{{ kubez_namespace }}"
filebeat_chart_version: 8.5.1

#################
# Kibana Options
#################
enable_kibana: "no"
kibana_name: kibana
kibana_namespace: "{{ kubez_namespace }}"
kibana_chart_version: 8.5.1

kibana_replicas: 1

##################
# Fluentd Options
##################
enable_fluentd: "no"
fluentd_name: fluentd
fluentd_namespace: "{{ kubez_namespace }}"
fluentd_chart_version: 0.3.9

###########################
# Kubez-autoscaler Options
###########################
enable_hpav2: "no"

##################
# Jenkins Options
##################
enable_jenkins: "no"

jenkins_namespace: "{{ kubez_namespace }}"
jenkins_storage_class: managed-nfs-storage
jenkins_storage_size: "8Gi"

# The initial password for admin
initial_admin_password: "admin123456"

#################
# Harbor Options
#################
enable_harbor: "no"
harbor_name: harbor
harbor_namespace: "{{ kubez_namespace }}"

harbor_storage_class: managed-nfs-storage
harbor_storage_size: "5Gi"
harbor_admin_password: "Harbor12345"

expose_http_nodeport: 30011
expose_notary_nodeport: 30012

# Setting it to "keep" to avoid removing PVCs during a helm delete
# operation. Leaving it empty will delete PVCs after the chart deleted
# (this does not apply for PVCs that are created for internal database
# and redis components, i.e. they are never deleted automatically)
harbor_resource_policy: " "

# Valid options are [ ingress, nodePort ]
expose_type: nodePort
expose_core_domain: core.harbor.kubez.com
expose_notary_domain: notary.harbor.kubez.com

####################################
# Operator-Lifecycle-Manager Options
####################################
enable_olm: "no"

################
# Istio Options
################
enable_istio: "no"
istio_chart_version: 1.16.1

####################
# Dashboard Options
####################
enable_dashboard: "no"
dashboard_chart_version: 6.0.0

dashboard_name: kubernetes-dashboard
dashboard_namespace: "{{ kubez_namespace }}"
dashboard_vip_address: ""
dashboard_node_port: 30001

##################
# Mariadb Options
##################
enable_mariadb: "no"
mariadb_name: mariadb
mariadb_namespace: "{{ kubez_namespace }}"
mariadb_storage_class: managed-nfs-storage
mariadb_storage_size: "8Gi"

###############
# Kong Options
###############
enable_kong: "no"
kong_name: kong
kong_namespace: "{{ kubez_namespace }}"
kong_storage_class: managed-nfs-storage

#################
# Redis Options
#################
enable_redis: "no"
redis_name: redis
redis_namespace: "{{ kubez_namespace }}"
redis_storage_class: managed-nfs-storage
redis_storage_size: "8Gi"

#################
# Consul Options
#################
enable_consul: "no"
consul_name: consul
consul_namespace: "{{ kubez_namespace }}"
consul_storage_class: managed-nfs-storage
consul_storage_size: "8Gi"

##########################
# Helm Chart Applications
##########################
enable_charts:
  - name: prometheus
    enabled: "{{ enable_prometheus | bool }}"
  - name: grafana
    enabled: "{{ enable_grafana | bool }}"
  - name: jenkins
    enabled: "{{ enable_jenkins | bool }}"
  - name: harbor
    enabled: "{{ enable_harbor | bool }}"
  - name: mariadb
    enabled: "{{ enable_mariadb | bool }}"
  - name: kong
    enabled: "{{ enable_kong | bool }}"
  - name: redis
    enabled: "{{ enable_redis | bool }}"
  - name: consul
    enabled: "{{ enable_consul | bool }}"
  - name: istio-base
    enabled: "{{ enable_istio | bool}}"
  - name: istiod
    enabled: "{{ enable_istio | bool}}"
  - name: dashboard
    enabled: "{{ enable_dashboard | bool }}"
  - name: elasticsearch
    enabled: "{{ enable_elasticsearch | bool }}"
  - name: kibana
    enabled: "{{ enable_kibana | bool }}"
  - name: filebeat
    enabled: "{{ enable_filebeat | bool }}"
  - name: fluentd
    enabled: "{{ enable_fluentd | bool }}"

charts:
  prometheus:
    name: prometheus
    namespace: "{{ kubez_namespace }}"
    repository:
      name: prometheus-community
      url: https://prometheus-community.github.io/helm-charts
    chart:
      path: prometheus-community/prometheus
      version: 15.9.2
    chart_extra_vars:
      server.persistentVolume.enabled: 'false' # 必须是字符串格式，否则会被 helm_toolbox 模块忽略
      alertmanager.persistentVolume.enabled: 'false'
      # 以 kube-state-metrics 为前缀设置子 chart kube-state-metrics 的属性
      kube-state-metrics.image.repository: jacky06/kube-state-metrics

  grafana:
    name: grafana
    namespace: "{{ kubez_namespace }}"
    repository:
      name: grafana
      url: https://grafana.github.io/helm-charts
    chart:
      path: grafana/grafana
      version: 6.29.6
    chart_extra_vars: {} # 如果没有 extra_vars，需留空

  jenkins:
    name: jenkins
    namespace: "{{ jenkins_namespace }}"
    repository:
      name: jenkinsci
      url: https://charts.jenkins.io/
    chart:
      path: jenkinsci/jenkins
      version: 4.2.20
    chart_extra_vars:
      persistence.storageClass: "{{ jenkins_storage_class }}"
      persistence.size: "{{ jenkins_storage_size }}"
      controller.adminPassword: "{{ initial_admin_password }}"

  harbor:
    name: "{{ harbor_name }}"
    namespace: "{{ harbor_namespace }}"
    repository:
      name: harbor
      url: https://helm.goharbor.io
    chart:
      path: harbor/harbor
      version: 1.11.0
    chart_extra_vars:
      expose.type: "{{ expose_type }}"
      expose.tls.enabled: 'false'
      harborAdminPassword: "{{ harbor_admin_password }}"
      expose.nodePort.ports.http.nodePort: "{{ '' if expose_type == 'ingress' else expose_http_nodeport }}"
      expose.nodePort.ports.notary.nodePort: "{{ '' if expose_type == 'ingress' else expose_notary_nodeport }}"
      expose.ingress.hosts.core: "{{ expose_core_domain if expose_type == 'ingress' else '' }}"
      expose.ingress.hosts.notary: "{{ expose_notary_domain if expose_type == 'ingress' else '' }}"
      persistence.persistentVolumeClaim.registry.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.chartmuseum.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.jobservice.scanDataExports.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.jobservice.jobLog.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.database.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.redis.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.trivy.storageClass: "{{ harbor_storage_class }}"
      persistence.persistentVolumeClaim.registry.size: "{{ harbor_storage_size }}"
      persistence.persistentVolumeClaim.chartmuseum.size: "{{ harbor_storage_size }}"
      persistence.persistentVolumeClaim.database.size: "{{ harbor_storage_size }}"
      persistence.persistentVolumeClaim.redis.size: "{{ harbor_storage_size }}"
      persistence.persistentVolumeClaim.trivy.size: "{{ harbor_storage_size }}"
      persistence.resourcePolicy: "{{ harbor_resource_policy }}"
      externalURL: "{{ 'https://' if expose_type == 'ingress' else 'http://' }}{{ expose_core_domain if expose_type == 'ingress' else hostvars[groups['kube-master'][0]]['ansible_' + network_interface]['ipv4']['address'] }}{{ '' if expose_type == 'ingress' else ':' }}{{ '' if expose_type == 'ingress' else expose_http_nodeport }}"

  mariadb:
    name: "{{ mariadb_name }}"
    namespace: "{{ mariadb_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/mariadb
      version: 11.4.3
    chart_extra_vars:
      global.storageClass: "{{ mariadb_storage_class }}"
      primary.persistence.storageClass: "{{ mariadb_storage_class }}"
      primary.persistence.size: "{{ mariadb_storage_size }}"
      secondary.persistence.storageClass: "{{ mariadb_storage_class }}"
      secondary.persistence.size: "{{ mariadb_storage_size }}"

  kong:
    name: "{{ kong_name }}"
    namespace: "{{ kong_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/kong
      version: 9.0.1
    chart_extra_vars:
      global.storageClass: "{{ kong_storage_class }}"
      ingressController.enabled: "false"
      postgresql.enabled: "false"

  redis:
    name: "{{ redis_name }}"
    namespace: "{{ redis_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/redis
      version: 17.4.2
    chart_extra_vars:
      global.storageClass: "{{ redis_storage_class }}"
      master.persistence.storageClass: "{{ redis_storage_class }}"
      master.persistence.size: "{{ redis_storage_size }}"
      replica.persistence.size: "{{ redis_storage_size }}"

  consul:
    name: "{{ consul_name }}"
    namespace: "{{ consul_namespace }}"
    repository:
      name: bitnami
      url: https://charts.bitnami.com/bitnami
    chart:
      path: bitnami/consul
      version: 10.9.9
    chart_extra_vars:
      global.storageClass: "{{ consul_storage_class }}"
      persistence.size: "{{ consul_storage_size }}"

  istio-base:
    name: istio-base
    namespace: istio-system
    repository:
      name: istio
      url: https://istio-release.storage.googleapis.com/charts
    chart:
      path: istio/base
      version: "{{ istio_chart_version }}"
    chart_extra_flags:
      - create-namespace
    chart_extra_vars: {}
  istiod:
    name: istiod
    namespace: istio-system
    repository:
      name: istio
      url: https://istio-release.storage.googleapis.com/charts
    chart:
      path: istio/istiod
      version: "{{ istio_chart_version }}"
    chart_extra_flags:
      - create-namespace
    chart_extra_vars: {}

  dashboard:
    name: "{{ dashboard_name }}"
    namespace: "{{ dashboard_namespace }}"
    repository:
      name: kubernetes-dashboard
      url: https://kubernetes.github.io/dashboard/
    chart:
      path: kubernetes-dashboard/kubernetes-dashboard
      version: "{{ dashboard_chart_version }}"
    chart_extra_vars: {}

  elasticsearch:
    name: "{{ elasticsearch_name }}"
    namespace: "{{ elasticsearch_namespace }}"
    repository:
      name: elastic
      url: https://helm.elastic.co
    chart:
      path: elastic/elasticsearch
      version: "{{ elasticsearch_chart_version }}"
    chart_extra_vars:
      replicas: "{{ elasticsearch_replicas }}"
      minimumMasterNodes: "{{ minimum_master_nodes }}"
      persistence.enabled: "false" # 暂时关闭，后续启用 storageclass
  kibana:
    name: "{{ kibana_name }}"
    namespace: "{{ kibana_namespace }}"
    repository:
      name: elastic
      url: https://helm.elastic.co
    chart:
      path: elastic/kibana
      version: "{{ kibana_chart_version }}"
    chart_extra_vars:
      replicas: "{{ kibana_replicas }}"
  filebeat:
    name: "{{ filebeat_name }}"
    namespace: "{{ filebeat_namespace }}"
    repository:
      name: elastic
      url: https://helm.elastic.co
    chart:
      path: elastic/filebeat
      version: "{{ filebeat_chart_version }}"
    chart_extra_vars: {}
  fluentd:
    name: "{{ fluentd_name }}"
    namespace: "{{ fluentd_namespace }}"
    repository:
      name: fluent
      url: https://fluent.github.io/helm-charts
    chart:
      path: fluent/fluentd
      version: "{{ fluentd_chart_version }}"
    chart_extra_vars: {}
